{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295abce",
   "metadata": {},
   "source": [
    "# Home Assignment 2 (30 points)\n",
    "\n",
    "Submit your solutions in teams of 5 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission (incorrect submissions get 0 points)!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points. We use the conda environment given below.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions.\n",
    "* Demonstrate that your code works by providing outputs for test inputs or with simple unit tests (`assert actual == expected`).\n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post on Slack, so we can clear up such questions for all students in the course.\n",
    "\n",
    "\n",
    "Conda environment used for grading:\n",
    "\n",
    "```yml\n",
    "name: nlp-ss2024\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pip\n",
    "  - pip:\n",
    "    - numpy==1.26.4\n",
    "    - nltk==3.8.1\n",
    "    - scikit-learn==1.4.2\n",
    "    - scipy==1.13.0\n",
    "    - notebook\n",
    "```\n",
    "\n",
    "\n",
    "### AI Disclosure\n",
    "\n",
    "Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.\n",
    "   - [ ] Yes, we used the following assistants: ....\n",
    "   - [ ] No \n",
    "\n",
    "---\n",
    "*(only complete the below questions if you answered yes above)*\n",
    "\n",
    "* If you used a large language model to assist you, please paste *all* of the prompts that you used below. Add a separate bullet for each prompt, and specify which problem is associated with which prompt.\n",
    "    * *your response here*\n",
    "* **Free response**: For each problem for which you used assistance, describe your overall experience with the AI. How helpful was it? Did it just directly give you a good answer, or did you have to edit it? Was its output ever obviously wrong or irrelevant? Did you use it to get the answer or check your own answer?\n",
    "    * *your response here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e66e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMR Usernames of all group members (<Username>@students.uni-marburg.de)\n",
    "team_members = [\n",
    "    \"Student1\",\n",
    "    \"Student2\",\n",
    "    \"...\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a19c09",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5e2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple, Sequence\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edf14c",
   "metadata": {},
   "source": [
    "### Task 1: WordNet word similarity\n",
    "\n",
    "In this task, we want to implement the similarity between two words in WordNet (https://www.nltk.org/api/nltk.corpus.reader.wordnet.html) using the NLTK package. The word similarity between two words is given by\n",
    "$$\n",
    "\\frac{1}{1+d}\n",
    "$$\n",
    "where $d$ is the distance of the shortest path in the hypernym/hyponym hierarchy tree in WordNet between any pair of synsets that are associated with the two input words.\n",
    "\n",
    "From NLTK you should __only__ use the `synsets`, `hypernyms` and `instance_hpyernyms` functions.\n",
    "\n",
    "The following subtasks build on each other, i.e. the functions of the preceding subtasks can be used for the current subtask.\n",
    "\n",
    "_Note: the distance of a synset to itself is 0, the distance to a direct hypernym is 1, ..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2b0d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wordnet \u001B[38;5;28;01mas\u001B[39;00m wn\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreader\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwordnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Synset\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6c82f",
   "metadata": {},
   "source": [
    "__a)__ Write a function ``shortest_paths_to`` that takes a synset as input and computes the shortest paths to all nodes on the way to the root in the hypernym hierarchy tree of WordNet. The function should return a dictionary that matches all visited hypernyms on the way(s) to the root to the distance of the shortest path from the input synset. Consider that a synset might have multiple paths to the root and that some nodes might appear in multiple paths. However, we only want to store the shortest distances. Moreover, keep in mind that the input synset might be an instance. \n",
    "\n",
    "Use the signature in the cell below.\n",
    "\n",
    "__Example:__ _Calling_ ``shortest_paths_to(s)`` _on the synset_ ``s = wn.synset('calculator.n.01')`` _should yield the following result:_\n",
    "\n",
    "``\n",
    "{Synset('calculator.n.01'): 0,\n",
    " Synset('expert.n.01'): 1,\n",
    " Synset('person.n.01'): 2,\n",
    " Synset('causal_agent.n.01'): 3,\n",
    " Synset('organism.n.01'): 3,\n",
    " Synset('physical_entity.n.01'): 4,\n",
    " Synset('living_thing.n.01'): 4,\n",
    " Synset('entity.n.01'): 5,\n",
    " Synset('whole.n.02'): 5,\n",
    " Synset('object.n.01'): 6}\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = wn.synset('calculator.n.01')\n",
    "s.hypernyms()\n",
    "#s.root_hypernyms() # [Synset('entity.n.01')]\n",
    "s.hypernym_paths() # path[0] is shorter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for path in s.hypernym_paths():\n",
    "    path.reverse()\n",
    "    print(type(path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy = wn.synset('person.n.01')\n",
    "nb = s.path_similarity(dummy)\n",
    "print(1/nb-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset\n",
    "    \"\"\"\n",
    "    shortest_path = {}\n",
    "\n",
    "    for path in start_syn.hypernym_paths():\n",
    "        for element in path:\n",
    "            distance = int(1/start_syn.path_similarity(element)-1)\n",
    "            shortest_path[element] = distance\n",
    "\n",
    "    shortest_path = dict(sorted(shortest_path.items(), key=lambda item: item[1]))\n",
    "    return shortest_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset\n",
    "    \"\"\"\n",
    "    shortest_path = {}\n",
    "\n",
    "    for path in start_syn.hypernym_paths():\n",
    "        path.reverse()\n",
    "        for index, element in enumerate(path):\n",
    "            distance = index\n",
    "            if element in shortest_path:\n",
    "                shortest_path[element] = min(shortest_path[element], distance)\n",
    "            else:\n",
    "                shortest_path[element] = distance\n",
    "\n",
    "    shortest_path = dict(sorted(shortest_path.items(), key=lambda item: item[1]))\n",
    "    return shortest_path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortest_paths_to(s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "3fda6f14",
   "metadata": {},
   "source": [
    "__b)__ Write a function ``merge_paths`` that gets two dictionaries that map synsets to shortest distances (you can assume they were created by the function from __a)__) and merges them. The function shold return a dictionary that includes all synsets and distances that appear in any of the input dictionaries. If a synset appears in both input dictionaries, we want to keep the shorter distance. Leave the input dictionaries unaltered.\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_paths(p1: Dict[Synset, int], p2: Dict[Synset, int]) -> Dict[Synset, int]:\n",
    "    \"\"\"Merge two paths keeping the shorter distance for synsets that appear more than once.\n",
    "    :param p1: first dict that maps synsets to their shortest distances\n",
    "    :param p2: second dict that maps synsets to their shortest distances\n",
    "    :return: merged dict\n",
    "    \"\"\"\n",
    "    merged_dict = p1.copy()\n",
    "\n",
    "    for key, value in p2.items():\n",
    "        if key in merged_dict:\n",
    "            if value < merged_dict[key]:\n",
    "                merged_dict[key] = value\n",
    "        else:\n",
    "            merged_dict[key] = value\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7ce46",
   "metadata": {},
   "source": [
    "__c)__ Write a function ``all_hypernym_paths`` that gets a word as input and returns a dictionary that maps all hypernyms that are reachable from the set of synsets associated with the word to the shortest distance leading there.\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13112dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word: str) -> Dict[Synset, int]:\n",
    "    \"\"\"Get all hypernyms of all synsets associated with the input word and compute the shortest distance leading there.\n",
    "    :param word: input word\n",
    "    :return: dict that matches all reachable hypernyms to their shortest distance\n",
    "    \"\"\"\n",
    "    synset = wn.synsets(word)\n",
    "    all_paths = {}\n",
    "    for s in synset:\n",
    "        all_paths = merge_paths(all_paths, shortest_paths_to(s))\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_hypernym_paths(\"calculator\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "49270f8b",
   "metadata": {},
   "source": [
    "__d)__  Write a function ``get_dist`` that returns the word similarity between two input words, according to the formula given in the task description at the beginning.  \n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(w1 : str, w2 : str) -> float:\n",
    "    \"\"\"Compute the similarity between two input words in the WordNet hierarchy tree.\n",
    "    :param w1: first input word\n",
    "    :param w2: second input word\n",
    "    :return: word similarity\n",
    "    \"\"\"\n",
    "    syn1 = wn.synsets(w1)\n",
    "    syn2 = wn.synsets(w2)\n",
    "    all_paths1 = {}\n",
    "    all_paths2 = {}\n",
    "    for s in syn1:\n",
    "        all_paths1 = merge_paths(all_paths1, shortest_paths_to(s))\n",
    "    for s in syn2:\n",
    "        all_paths2 = merge_paths(all_paths2, shortest_paths_to(s))\n",
    "    common = set(all_paths1.keys()).intersection(set(all_paths2.keys()))\n",
    "    if len(common) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/(1+min([all_paths1[s] for s in common]+[all_paths2[s] for s in common]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_dist(\"calculator\", \"chewing_gum\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "3426121d",
   "metadata": {},
   "source": [
    "### Task 2: Lesk algorithm\n",
    "\n",
    "In this task we want to implement a simple version of the Lesk algorithm, a thesaurus-based method for word sense disambiguation. Given a target word $w$ and a context, the algorithm finds the word sense that is most fitting in the context. To achieve this, the Lesk algorithm computes the number of overlapping words between the context sentence and the definitions of the WordNet synsets, associated with $w$.\n",
    "\n",
    "Write a function ``lesk`` that takes a word and a context string (e.g. a sentence) and returns the most fitting sense from the synsets associated with the word and the corresponding context overlap. The most fitting sense is the one whose definition shares the most words with the context string. Before matching tokens, make sure to \n",
    "\n",
    "* only include valid tokens (cf. HA 1, task 2a)\n",
    "* remove stopwords\n",
    "* only match stems of words (e.g. consider the ``PorterStemmer`` from ``nltk``)\n",
    "\n",
    "When computing the context overlap, count each stemmed word only once, even if they occur multiple times. If there is no fitting synset, i.e. the context overlap between the context and the synset definitions is 0, return None instead.\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HA 1, task 2a)\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :param remove_stopwords: bool indicating if stopwords should be removed\n",
    "                             False by default\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    punct = string.punctuation\n",
    "    stop = stopwords.words('english')\n",
    "    digit = re.compile(r\"\\d+\")\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in punct:\n",
    "            continue\n",
    "        if remove_stopwords and t.lower() in stop:\n",
    "            continue\n",
    "        if re.fullmatch(digit, t):\n",
    "            continue\n",
    "        valid.append(t.lower())\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lesk(word: str, context: str) -> (Synset, int):\n",
    "    '''\n",
    "    Compute the most probable sense of a word in the given context.\n",
    "    :param word: ambiguous word\n",
    "    :param context: context in which the word appears\n",
    "    :returns:\n",
    "        - synset with the most likely word sense\n",
    "        - context overlap of synset and context\n",
    "    '''\n",
    "\n",
    "    valid_context = get_valid_tokens(nltk.word_tokenize(context), remove_stopwords=True)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    valid_context = [stemmer.stem(t) for t in valid_context]\n",
    "    synsets = wn.synsets(word)\n",
    "    best_synset = None\n",
    "    best_overlap = 0\n",
    "    for s in synsets:\n",
    "        definition = get_valid_tokens(nltk.word_tokenize(s.definition()), remove_stopwords=True)\n",
    "        definition = [stemmer.stem(t) for t in definition]\n",
    "        overlap = len(set(definition).intersection(set(valid_context)))\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            best_synset = s\n",
    "    if best_overlap == 0:\n",
    "        return None\n",
    "    return best_synset, best_overlap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = \"The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities.\"\n",
    "lesk(\"bank\", test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "1f37f1ef",
   "metadata": {},
   "source": [
    "### Task 3: Minimum cost string alignment\n",
    "\n",
    "In this tak we want to compute an alignment between two strings, that has minimum edit distance. \n",
    "\n",
    "Implement a function that takes two strings and their edit distance matrix and returns a minimum cost alignment. You can assume that the edit distance matrix is provided by the function that you implemented in exercise 3, task 2, with a substitution cost of 2. \n",
    "\n",
    "A minimum cost alignment consists of two strings that, printed below each other comprise the alignment, where insertions and deletions are represented by a ``*``. Use the function signature in the cell below.\n",
    "\n",
    "__Example:__ _Given the input strings_ `\"INTENTION\"` _and_ `\"EXECUTION\"` _and the corresponding edit distance matrix:_\n",
    "\n",
    "```\n",
    "[[ 0  1  2  3  4  5  6  7  8  9]\n",
    " [ 1  2  3  4  5  6  7  6  7  8]\n",
    " [ 2  3  4  5  6  7  8  7  8  7]\n",
    " [ 3  4  5  6  7  8  7  8  9  8]\n",
    " [ 4  3  4  5  6  7  8  9 10  9]\n",
    " [ 5  4  5  6  7  8  9 10 11 10]\n",
    " [ 6  5  6  7  8  9  8  9 10 11]\n",
    " [ 7  6  7  8  9 10  9  8  9 10]\n",
    " [ 8  7  8  9 10 11 10  9  8  9]\n",
    " [ 9  8  9 10 11 12 11 10  9  8]]\n",
    "```\n",
    "  \n",
    "_your function should return the two strings_ ``INTE***NTION`` _and_ ``***EXECUTION`` _that represent the alignment, when printed below each other:_\n",
    " \n",
    " ``INTE***NTION``    \n",
    " ``***EXECUTION`` \n",
    " \n",
    " __Remark:__ _The alignment in the example above is not the only solution. In this task all alignments with minimum edit distance are accepted._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd021e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(str1: str, str2: str, D: NDArray[NDArray[int]]) -> tuple[str, str]:\n",
    "    '''\n",
    "    :param str1: first string for alignment\n",
    "    :param str2: second string for alignment\n",
    "    :param D: edit distance matrix of str1 and str2\n",
    "    :returns: tuple of strings that indicate the alignment of the input strings\n",
    "    '''\n",
    "\n",
    "    alignment1 = \"\"\n",
    "    alignment2 = \"\"\n",
    "    i = len(str1)\n",
    "    j = len(str2)\n",
    "    while i > 0 and j > 0:\n",
    "        min_cur = min(D[i-1][j-1], D[i][j-1], D[i-1][j])\n",
    "        if min_cur == D[i-1][j-1] and str1[i-1] == str2[j-1]:\n",
    "            alignment1 = str1[i-1] + alignment1\n",
    "            alignment2 = str2[j-1] + alignment2\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif min_cur == D[i-1][j-1]:\n",
    "            alignment1 = '<'+ str1[i-1] +'>'+ alignment1\n",
    "            alignment2 = '<'+ str2[j-1] +'>'+ alignment2\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif min_cur == D[i-1][j] + 1:\n",
    "            alignment1 = str1[i-1] + alignment1\n",
    "            alignment2 = \"*\" + alignment2\n",
    "            i -= 1\n",
    "        else:\n",
    "            alignment1 = \"*\" + alignment1\n",
    "            alignment2 = str2[j-1] + alignment2\n",
    "            j -= 1\n",
    "    while i > 0:\n",
    "        alignment1 = str1[i-1] + alignment1\n",
    "        alignment2 = \"*\" + alignment2\n",
    "        i -= 1\n",
    "    while j > 0:\n",
    "        alignment1 = \"*\" + alignment1\n",
    "        alignment2 = str2[j-1] + alignment2\n",
    "        j -= 1\n",
    "    return alignment1, alignment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Exercise_03.edit_distance import edit_distance\n",
    "array= edit_distance(\"autohaus\", \"baumhaus\")\n",
    "print(array[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "str1, str2 = get_alignment(\"autohaus\", \"baumhaus\", array[1])\n",
    "print(str1)\n",
    "print(str2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
